[
  {
    "objectID": "posts/2023-02-13-Research-Week-With-Google.html",
    "href": "posts/2023-02-13-Research-Week-With-Google.html",
    "title": "Research Week With Google 2023",
    "section": "",
    "text": "Google Research India conducts reseach week, where students are exposed to latest research happenings in Computer Vision, NLP, Machine Learning and alied areas. Applications are invited during year end, typically during November/December months. I was selected and hence had the opportunity to attend “Research Week With Google 2023”, in Computer Vision B track from 29-31 January, 2023.\nBelow is the summary of the talks and my experiences specifically related to Computer Vision B track.\n\nDay1\nKeyNote : The day was started with a key-note by Prateek and Partha. An overview of the program along with different reseach areas in ML related to OOD, specifically simplicity Bias was provided by Prateek. Partha covered different aspects of NLP and challenges related to Multilingual and Multimodal aspects. Fairness implications related to NLP were also discussed. Finally, interesting research directions were laid out to the audience.\nRobustifying Deep Neural Networks Talk : This talks was given by Prof. R. Venkatesth Babu in which he discussed about efficient algorithms for adversarial training. Other topics such as Single Step adversarial training with drop-out scheduling, Towards achieving adversarial robustness by enforcing feature consistency across bit planes were also discussed. The session was highly stimulating and insightful.\nContinual Query Releases Talk : This session was giveny by Prof. Himanshu Tyagi. Insighful details were provided about privacy techniques, epsilon differential privacy their definitions, applications specifically in the context of Continual Query Releases. This talk underscored the importance and the need for differential privacy particulary within the context of current deep learning algorithms.\nMachine Learning UsesCases : In this last part of the session Gaurav Aggarwal presented different use cases that are very relevant to Indian context. Some of them which inspired me are using machine learning for satellite images and analysing hand written prescriptions.\nThe socials were enjoyable and helped connect with different speakers.\nMe and Harsh on Day1\n\n\n\n\nDay2\nKeyNote : In this keynote on Day2, Doina Precup covered various aspects of Reinforcement Learning with a focus on Synthesizing new behaviour with combinations of reward functions, continual reinforcement learning and agent interactions with humans among others. This talks is high informative and helped delve into the world of reinforcement learning and its applications.\nMemory Safety and the case of modular VM development : This talk given by Steve Blackburn is one of my favourite. This covered various aspects of low level details that are of importance to garbage collection and virtual machines. This talks really inspired to look at low level system details that can have huge impact at web scale.\nVision Models, OOD and Others : In this talk by Pradeep Shenoy, various techniques and recent work that address OOD, Multi-object Multi-Part Segmentation i.e. compositionality, learning under slow concept drift, selective classification & domain adapation and adaptive mixing of losses were presented.\nMe with Steve Blackburn on Day2 during Socials\n\n\n\nDay3\nKeyNote : In the last day of the keynote Jeff Dean presented the current trends of Deep Learning. Some of these include LLMs, Generative Models, Model Scaling, Single Model that can generalize across modalities/tasks, efficient sparse mdoels, computational photography using DL, Impact in Engineering, Science, Health and Sustainability, Deeper and broader understanding of ML and finally Principles for responsible AI. This gave a bird’s eye view of different facets and directions that one can undertake research broadly. This talk is extremely interesting and thought provoking.\nML for Climate, Earth Observation, Weather etc. : In this talk by Varun Gulshan, the usage of deep learning techniques for weather prediction, climate change were presented. This is an important application and provided interesting direction from an application perspective for weather and climate sciences.\nIs Classification All you need for computer vision : In this final talk by Prof. Angela Yao, professor covered various facets of losses, how, why and the intricacies of them in the context of classification. Although this talk is theoritical in nature, this has helped my understanding the intricacies of loss functions and their workings in deep learning.\nWith everyone on Final Day\n\nAlong with this, fireside chats and panel discussions helped the audience understand various career options and research as a career choice.\nIt was a wonderful experience attending “Research Week With Google 2023”. This event helped me understand the latest happenings, current directions and answer questions on various topics. This is a an opportunity that no one can miss. I highly enjoyed the event.\nFeel free to go though the Research Week with Google 2023 website for more details.\nReferences :\n\nResearch Week with Google 2023\nResearch Week with Google - officially a wrap!\nResearch Week w/ Google\nResearch Week w/ Google Apply!!!"
  },
  {
    "objectID": "posts/2020-08-24-Self-Supervised-Learning.html",
    "href": "posts/2020-08-24-Self-Supervised-Learning.html",
    "title": "Introduction to Self-Supervised Learning",
    "section": "",
    "text": "In general , if we observe the way humans learn it is evident that they don’t need huge training data with explicit labels. Most of the learning is done largely in an unsupervised fashion i.e. without labels. Hence it is natural to explore if unsupervised learning can be used to improve the efficiency of different deep learning algorithms. Technically, this can mean if unsupervised learning can be used to improve or learn more interesting representations from unlabelled data.\nLets look at some of the ideas of Supervised Learning, Unsupervised Learning and Self-Supevised Learning below.\nSupervised Learning : This refers to the process of training learning algorithms where an inhererent supervision is available in the data. Futher this can be defined as using labels available in the data to train learning algorithms. Classification is a common example for Supervised Learning.\nUnsupervised Learning : In Unsupervised learning, there are no labels available in the data . Instead the data with features is just used by the algorithm to learn inherent structure. Clustering is an example of the algorithm that uses Unsupervised Learning.\nSelf-Supervised Learning : This can be thought of a subset of Unsupervised learning where in own supervision is created via pretext tasks or proxy loss. The idea and the hope is that the pretext task or proxy loss helps to learn better representations thus improving the accuracy of the learning algorithms. Further to this , it is also believed that this helps the learning algorithms generalize better.\nBelow are some of the advantages of using self-supervised learning:\n\nAvailability of huge amount of unlabelled data for training\nLabel once and utilize for a variety of different tasks\nEmulate the natural way in which humans/animals learn\n\nIn the next blog post, we will look at some interesting literature along with concepts that help us dive deeper into self-supervised learning.\nReferences :\n\nhttps://youtu.be/dMUes74-nYY\nhttps://sites.google.com/view/berkeley-cs294-158-sp20/home\n“Lecture 7 Self-Supervised Learning” of CS294-158-SP20 course"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ai-notes",
    "section": "",
    "text": "Research Week With Google 2023\n\n\n\n\n\n\n\nResearch Week\n\n\nAI Research\n\n\n\n\nResearch Week Experience\n\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Self-Supervised Learning\n\n\n\n\n\n\n\nself-supervised learning\n\n\nunsupervised learning\n\n\n\n\nBasics of Self-Supervised Learning\n\n\n\n\n\n\nAug 24, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "AI Notes",
    "section": "",
    "text": "I am a data scientist with deep interest in Computer Vision, Artificial Intelligence, Machine Learning and Deep Learning.\nI am passionate about technology and its potential to solve variety of problems. This blog is a way for me to explain concepts which I have learnt along the way. Along with this I hope this blog also acts as a reference for me in future."
  }
]